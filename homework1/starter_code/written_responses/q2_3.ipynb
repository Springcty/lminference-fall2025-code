{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6deff0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyuca/miniconda3/envs/IntroToML/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd203ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:37<00:00,  9.46s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9697c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(prompt, max_new_tokens=64):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # greedy decoding\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "    \n",
    "    generated_ids = outputs.sequences[0]\n",
    "    prompt_len = inputs.input_ids.shape[1]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute logits for loss\n",
    "    with torch.no_grad():\n",
    "        logits = model(generated_ids.unsqueeze(0)).logits\n",
    "\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = generated_ids.unsqueeze(0)[:, 1:].contiguous()\n",
    "    \n",
    "    # Start computing loss from the end of the prompt\n",
    "    shift_logits = shift_logits[:, prompt_len-1:, :]\n",
    "    shift_labels = shift_labels[:, prompt_len-1:]\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1))\n",
    "    loss_per_token = loss.view(shift_labels.size())\n",
    "\n",
    "    # Per-token perplexity\n",
    "    ppl_per_token = torch.exp(loss_per_token).squeeze().tolist()\n",
    "\n",
    "    # Global perplexity\n",
    "    global_ppl = math.exp(loss.mean().item())\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_text\": generated_text,\n",
    "        \"per_token_ppl\": ppl_per_token,\n",
    "        \"global_ppl\": global_ppl\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa91ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyuca/miniconda3/envs/IntroToML/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/tianyuca/miniconda3/envs/IntroToML/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prompt: 2 + 2 = \n",
      "Generated: 2 + 2 = 4. 2 + 2 = 4. 2 + 2 = 4. 2 + 2 = 4. 2 + 2 = 4. 2 + 2 = 4. 2 + 2 = 4. 2 + 2 = 4...\n",
      "Global perplexity: 1.21\n",
      "First 10 per-token PPL: [2.498046875, 6.828125, 13.40625, 4.53515625, 2.779296875, 1.0556640625, 1.734375, 1.451171875, 1.12109375, 2.40625]\n",
      "============================================================\n",
      "Prompt: The capital of China is \n",
      "Generated: The capital of China is 1,500 miles from the nearest ocean. The city is located in the north of the country, in the middle of the North China Plain. The city is located in the north of the country, in...\n",
      "Global perplexity: 2.07\n",
      "First 10 per-token PPL: [10.296875, 1.3212890625, 6.765625, 2.853515625, 2.294921875, 3.28515625, 9.921875, 2.884765625, 2.697265625, 9.140625]\n",
      "============================================================\n",
      "Prompt: Once upon a time, there was a \n",
      "Generated: Once upon a time, there was a 3-year-old boy named Jack. He was a very happy boy, and he loved to play with his toys. One day, Jack was playing with his toy car when he accidentally knocked over a vas...\n",
      "Global perplexity: 2.22\n",
      "First 10 per-token PPL: [20.328125, 3.76953125, 1.1943359375, 3.265625, 3.28515625, 71.5, 2.228515625, 3.11328125, 2.93359375, 4.10546875]\n",
      "============================================================\n",
      "Prompt: The message is: qwerf23jdaf0klsaf\n",
      "Generated: The message is: qwerf23jdaf0klsaf0klsaf0klsaf0klsaf0klsaf0klsaf0klsaf0klsaf0klsaf0klsaf0klsaf0klsaf0klsaf0klsaf0klsaf0klsaf0klsaf...\n",
      "Global perplexity: 1.18\n",
      "First 10 per-token PPL: [10.1015625, 7.40234375, 3.25, 3.556640625, 2.056640625, 2.3515625, 1.1220703125, 1.3046875, 1.19140625, 1.322265625]\n",
      "============================================================\n",
      "Prompt: The secret of the pen is \n",
      "Generated: The secret of the pen is 3D printing\n",
      "The secret of the pen is 3D printing\n",
      "The secret of the pen is 3D printing\n",
      "The secret of the pen is 3D printing\n",
      "The secret of the pen is 3D printing\n",
      "The secret of t...\n",
      "Global perplexity: 1.41\n",
      "First 10 per-token PPL: [11.3359375, 6.71484375, 2.916015625, 1.8642578125, 7.078125, 6.46484375, 1.0458984375, 1.0322265625, 1.0126953125, 1.029296875]\n",
      "============================================================\n",
      "Prompt: The password for the ancient library is \n",
      "Generated: The password for the ancient library is 1234567890. The password for the ancient library is 1234567890. The password for the ancient library is 1234567890. The password for the ancient library is 1234...\n",
      "Global perplexity: 1.24\n",
      "First 10 per-token PPL: [11.0703125, 2.86328125, 2.833984375, 2.556640625, 3.38671875, 9.453125, 5.0625, 1.646484375, 1.0732421875, 2.529296875]\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"2 + 2 = \",\n",
    "    \"The capital of China is \",\n",
    "    \"Once upon a time, there was a \",\n",
    "    \"The message is: qwerf23jdaf0klsaf\",\n",
    "    \"The secret of the pen is \",\n",
    "    \"The password for the ancient library is \"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for p in prompts:\n",
    "    res = compute_perplexity(p)\n",
    "    results.append(res)\n",
    "\n",
    "for r in results:\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Prompt: {r['prompt']}\")\n",
    "    print(f\"Generated: {r['generated_text'][:200]}...\")\n",
    "    print(f\"Global perplexity: {r['global_ppl']:.2f}\")\n",
    "    print(f\"First 10 per-token PPL: {r['per_token_ppl'][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28caec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntroToML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
